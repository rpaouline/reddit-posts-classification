{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "69b9a648-bcc7-490d-9f9b-ea244d156bd6"
   },
   "source": [
    "# Using Reddit's API for Predicting Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-23T19:28:02.619411Z",
     "start_time": "2017-10-23T19:28:02.600856Z"
    }
   },
   "source": [
    "In this project, we will practice two major skills. Collecting data via an API request and then building a binary predictor.\n",
    "\n",
    "As we discussed in week 2, and earlier today, there are two components to starting a data science problem: the problem statement, and acquiring the data.\n",
    "\n",
    "For this article, your problem statement will be: _What characteristics of a post on Reddit contribute most to what subreddit it belongs to?_\n",
    "\n",
    "Your method for acquiring the data will be scraping threads from at least two subreddits. \n",
    "\n",
    "Once you've got the data, you will build a classification model that, using Natural Language Processing and any other relevant features, predicts which subreddit a given post belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "a948d79c-5527-4c0d-ab23-f5d43ce72056"
   },
   "source": [
    "### Scraping Thread Info from Reddit.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up a request (using requests) to the URL below. \n",
    "\n",
    "*NOTE*: Reddit will throw a [429 error](https://httpstatuses.com/429) when using the following code:\n",
    "```python\n",
    "res = requests.get(URL)\n",
    "```\n",
    "\n",
    "This is because Reddit has throttled python's default user agent. You'll need to set a custom `User-agent` to get your request to work.\n",
    "```python\n",
    "res = requests.get(URL, headers={'User-agent': 'YOUR NAME Bot 0.1'})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "from bs4 import BeautifulSoup \n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"http://www.reddit.com/r/boardgames.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(URL, headers={'User-agent': 'Polina Bot 0.1'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use `res.json()` to convert the response into a dictionary format and set this to a variable. \n",
    "\n",
    "```python\n",
    "data = res.json()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "data = res.json()\n",
    "print(data['data'].__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting more results\n",
    "\n",
    "By default, Reddit will give you the top 25 posts:\n",
    "\n",
    "```python\n",
    "print(len(data['data']['children']))\n",
    "```\n",
    "\n",
    "If you want more, you'll need to do two things:\n",
    "1. Get the name of the last post: `data['data']['after']`\n",
    "2. Use that name to hit the following url: `http://www.reddit.com/r/boardgames.json?after=THE_AFTER_FROM_STEP_1`\n",
    "3. Create a loop to repeat steps 1 and 2 until you have a sufficient number of posts. \n",
    "\n",
    "*NOTE*: Reddit will limit the number of requests per second you're allowed to make. When you create your loop, be sure to add the following after each iteration.\n",
    "\n",
    "```python\n",
    "time.sleep(3) # sleeps 3 seconds before continuing```\n",
    "\n",
    "This will throttle your loop and keep you within Reddit's guidelines. You'll need to import the `time` library for this to work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server returned after=None\n",
      "Server returned after=None\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# check if my file exists\n",
    "project_data = Path(\"./project_data.csv\")\n",
    "\n",
    "if project_data.is_file():  \n",
    "    # if does, read it\n",
    "    df = pd.read_csv('./project_data.csv',index_col=0)\n",
    "    \n",
    "else:\n",
    "    # else read data from server\n",
    "    all_posts =[]\n",
    "    for subreddit in ['cats','dogs']:\n",
    "        url = 'http://www.reddit.com/r/'+subreddit+'.json'\n",
    "        num_posts = 0\n",
    "        while num_posts < 1000: \n",
    "            # construct a list of 500 for each subreddit\n",
    "\n",
    "            # Get the posts by hitting the url, put it in json and store it\n",
    "            res = requests.get(url, headers={'User-agent': 'polina'})\n",
    "            data = res.json()\n",
    "            \n",
    "            # save only the posts out of the json into the list_of_posts, then\n",
    "            # add all the posts to the all_posts list\n",
    "            list_of_posts = data['data']['children']\n",
    "            all_posts = all_posts + list_of_posts\n",
    "            num_posts += len(list_of_posts)\n",
    "            \n",
    "            #print('The current number of posts: ', num_posts)    \n",
    "\n",
    "            # reassign the after to the current 'after', and then update the url to hit\n",
    "            if data['data']['after'] == None:\n",
    "                # something goes wrong\n",
    "                print('Server returned after=None')\n",
    "                break\n",
    "            else:   \n",
    "                after = data['data']['after']\n",
    "                url = 'http://www.reddit.com/r/'+subreddit+'.json?after=' + after\n",
    "\n",
    "            # go to sleep for 3 seconds so you do not overwhelm reddit and get kicked out\n",
    "\n",
    "            time.sleep(3)\n",
    "    \n",
    "    # extract data    \n",
    "    all_posts_data = [x['data']['selftext'] for x in all_posts]\n",
    "    # extract titles\n",
    "    all_posts_titles = [x['data']['title'] for x in all_posts]\n",
    "    all_posts_subreddit = [x['data']['subreddit'] for x in all_posts]\n",
    "    \n",
    "    # extract descriptions, and remember, that not all of them have description\n",
    "    for i in range(len(all_posts)):\n",
    "        try:\n",
    "            all_posts_titles[i] += all_posts[i]['data']['secure_media']['oembed']['description']\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # create dataframe\n",
    "    df = pd.DataFrame(all_posts_data,columns=['data'])\n",
    "    df['title'] = all_posts_titles\n",
    "    df['subreddit'] = all_posts_subreddit\n",
    "    \n",
    "    # Export to csv\n",
    "    df.to_csv('project_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "43e71edd-210e-42b1-9336-70a931f048af"
   },
   "source": [
    "### Save your results as a CSV\n",
    "You may do this regularly while scraping data as well, so that if your scraper stops of your computer crashes, you don't lose all your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "783fd153-28ac-47ab-bfca-27e7c1de95b4"
   },
   "source": [
    "> We saved to csv above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "db045898-1d2d-4af2-8e79-437c4c7546b4"
   },
   "source": [
    "## NLP\n",
    "\n",
    "#### Use `CountVectorizer` or `TfidfVectorizer` from scikit-learn to create features from the thread titles and descriptions (NOTE: Not all threads have a description)\n",
    "- Examine using count or binary features in the model\n",
    "- Re-evaluate your models using these. Does this improve the model performance? \n",
    "- What text features are the most valuable? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Do some preprocessing:\n",
    "- Lead our text to lower case\n",
    "- Leave letters only\n",
    "- Use lemmatizer\n",
    "- Delete stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(raw_text):\n",
    "    \n",
    "    # 1. Convert to lowercase\n",
    "    lower_text = raw_text.lower()\n",
    "    \n",
    "    # 2. Remove punctuation\n",
    "    letters_only = re.sub(\"[^a-z]\",     # The pattern to search for\n",
    "                          \" \",          # The pattern to replace it with\n",
    "                          lower_text )  # The text to search\n",
    "    \n",
    "    # 3. Split and lemmatize words\n",
    "    words = letters_only.split()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words_lem = [lemmatizer.lemmatize(i) for i in words]\n",
    "     \n",
    "    # 4. Remove stop words\n",
    "    stops = set(stopwords.words('english'))\n",
    "    meaningful_words = [w for w in words if w not in stops]\n",
    "\n",
    "    \n",
    "    # 5. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return (' '.join(meaningful_words))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Create new feature with preparing title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['prep_title']=df['title'].map(preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Create a target column from subreddit: 1 if it 'boardgames' and 0 else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>prep_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>Please do us mods a favor and if you report so...</td>\n",
       "      <td>cats</td>\n",
       "      <td>please us mods favor report something claiming...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>Drew a nice picture of a cat on my iPad the ot...</td>\n",
       "      <td>cats</td>\n",
       "      <td>drew nice picture cat ipad day black backgroun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>This is my feline. There is no tragic protect ...</td>\n",
       "      <td>cats</td>\n",
       "      <td>feline tragic protect story definitely youthfu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>His name is Meow Meow. He is my pride and joy....</td>\n",
       "      <td>cats</td>\n",
       "      <td>name meow meow pride joy years old ever since ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>My babies. Just wanted to celebrate the love I...</td>\n",
       "      <td>cats</td>\n",
       "      <td>babies wanted celebrate love girls probably re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  data                                              title subreddit  \\\n",
       "0       Please do us mods a favor and if you report so...      cats   \n",
       "1       Drew a nice picture of a cat on my iPad the ot...      cats   \n",
       "2       This is my feline. There is no tragic protect ...      cats   \n",
       "3       His name is Meow Meow. He is my pride and joy....      cats   \n",
       "4       My babies. Just wanted to celebrate the love I...      cats   \n",
       "\n",
       "                                          prep_title  \n",
       "0  please us mods favor report something claiming...  \n",
       "1  drew nice picture cat ipad day black backgroun...  \n",
       "2  feline tragic protect story definitely youthfu...  \n",
       "3  name meow meow pride joy years old ever since ...  \n",
       "4  babies wanted celebrate love girls probably re...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'] = df['subreddit'].map(lambda x: 1 if x=='dogs' else 0)\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Create X from 'title' column by CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer(stop_words='english')\n",
    "\n",
    "cvec_X_data = cvec.fit_transform(df['prep_title'])\n",
    "\n",
    "cvec_X  = pd.DataFrame(cvec_X_data.todense(),\n",
    "                   columns=cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Use cross-validation and logistic regression to estimate result of count vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logreg score for CountVectorizer:  0.9779403763531642\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True,random_state=42)\n",
    "logreg = LogisticRegression(random_state=42)\n",
    "print('Logreg score for CountVectorizer: ',cross_val_score(logreg,cvec_X,y,cv=kf).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Create X from 'prep_title' column by TfidVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfid = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "tfid_X_data = tfid.fit_transform(df['prep_title'])\n",
    "\n",
    "tfid_X  = pd.DataFrame(tfid_X_data.todense(),\n",
    "                   columns=cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Use cross-validation and logistic regression to estimate result of Tfid vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logreg score for TfidVectorizer:  0.9699742969175063\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(random_state=42)\n",
    "print('Logreg score for TfidVectorizer: ',cross_val_score(logreg,tfid_X,y,cv=kf).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "04563b69-f7b6-466f-9d65-fc62c9ddee6a"
   },
   "source": [
    "## Predicting subreddit using Random Forests + Another Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "c7631f51-07f2-4c79-a093-3e9bc7849a48"
   },
   "source": [
    "#### We want to predict a binary variable - class `0` for one of your subreddits and `1` for the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We already create out target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "a7afb2c0-d41e-4779-8216-91cd8dd4473f"
   },
   "source": [
    "#### Thought experiment: What is the baseline accuracy for this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "focus": false,
    "id": "87a17d3d-b7f4-4747-9f75-f9af1d18a174"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.541054\n",
       "1    0.458946\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "4fb29de2-5b98-474c-a4ad-5170b72b9aea"
   },
   "source": [
    "#### Create a `RandomForestClassifier` model to predict which subreddit a given post belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "588f9845-6143-4bcc-bfd1-85d45b79303d"
   },
   "source": [
    "> Use random forest on both cvec and tfid samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest score for CountVectorizer:  0.9791729986304196\n"
     ]
    }
   ],
   "source": [
    "cvec_rf = RandomForestClassifier(random_state=42)\n",
    "print('Random forest score for CountVectorizer: ',cross_val_score(cvec_rf,cvec_X,y,cv=kf).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest score for TfidVectorizer:  0.9816269863604811\n"
     ]
    }
   ],
   "source": [
    "tfid_rf = RandomForestClassifier(random_state=42)\n",
    "print('Random forest score for TfidVectorizer: ',cross_val_score(tfid_rf,tfid_X,y,cv=kf).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "9367beff-72ba-4768-a0ba-a50b335de61d"
   },
   "source": [
    "#### Use cross-validation in scikit-learn to evaluate the model above. \n",
    "- Evaluate the accuracy of the model, as well as any other metrics you feel are appropriate. \n",
    "- **Bonus**: Use `GridSearchCV` with `Pipeline` to optimize your `CountVectorizer`/`TfidfVectorizer` and classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "269b9e7c-60b5-4a06-8255-881d7395bc1b"
   },
   "source": [
    "> We did it above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repeat the model-building process using a different classifier (e.g. `MultinomialNB`, `LogisticRegression`, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Use multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB score for CountVectorizer:  0.9638562128290274\n"
     ]
    }
   ],
   "source": [
    "cvec_nb = MultinomialNB()\n",
    "print('Multinomial NB score for CountVectorizer: ',cross_val_score(cvec_nb,cvec_X,y,cv=kf).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB score for TfidVectorizer:  0.9607887281664509\n"
     ]
    }
   ],
   "source": [
    "tfid_nb = MultinomialNB()\n",
    "print('Multinomial NB score for TfidVectorizer: ',cross_val_score(tfid_nb,tfid_X,y,cv=kf).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can see that Random Forest much better on both cvec and tfid samples, than multinomial NB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Use support vectors machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM score for CountVectorizer 0.9822303921568627\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'kernel': 'linear'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "svc = svm.SVC()\n",
    "params = {\n",
    "     'kernel':['rbf','linear','poly','sigmoid']\n",
    "\n",
    "}\n",
    "gs = GridSearchCV(svc,param_grid=params)\n",
    "gs.fit(cvec_X,y)\n",
    "print(\"SVM score for CountVectorizer\",gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Do the same but for TfidVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM score for TfidVectorizer 0.9822303921568627\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'C': 1, 'kernel': 'linear'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = svm.SVC()\n",
    "params = {\n",
    "    'kernel':['linear','poly'],\n",
    "    'C':[0.5,1,1.5]\n",
    "\n",
    "}\n",
    "gs = GridSearchCV(svc,param_grid=params)\n",
    "gs.fit(tfid_X,y)\n",
    "print(\"SVM score for TfidVectorizer\",gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Use decision tree for CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree score for CountVectorizer:  0.9834618487458021\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "cvec_tree = DecisionTreeClassifier(random_state = 42)\n",
    "print('Decision tree score for CountVectorizer: ',cross_val_score(cvec_tree,cvec_X,y,cv=kf).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Do the same for TfidVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree score for CountVectorizer:  0.9840734695409091\n"
     ]
    }
   ],
   "source": [
    "tfid_tree = DecisionTreeClassifier(random_state = 42)\n",
    "print('Decision tree score for CountVectorizer: ',cross_val_score(tfid_tree,tfid_X,y,cv=kf).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executive Summary\n",
    "---\n",
    "Put your executive summary in a Markdown cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that two categories 'cats' and 'dogs' may be divided with very high accuracy. All the methods give us pretty high accuracy (logistic regression, multinomial NB, random forest, decision tree, support vectors) on the both samples - count vectorized and Tfold vectorized. But the highest score has decision tree classifier with the count vectorizer.\n",
    "For 'games' and 'boardgames' the best result is by the multinomial NB on count vectorized data, acc. 89%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
